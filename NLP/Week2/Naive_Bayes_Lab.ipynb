{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447b35d3",
   "metadata": {},
   "source": [
    "# Naive Bayes Sentiment Analysis on Tweets\n",
    "\n",
    "Welcome to my learning notebook for Naive Bayes sentiment analysis! In this notebook, I implement a full pipeline for classifying tweets as positive or negative using NLP techniques and a custom Naive Bayes model. All code and explanations are my own, stepwise, and educational for sharing on GitHub.\n",
    "\n",
    "**Outline:**\n",
    "1. Import Required Libraries and Data\n",
    "2. Process the Data\n",
    "3. Implement Helper Functions\n",
    "4. Train Naive Bayes Model\n",
    "5. Test Naive Bayes Model\n",
    "6. Analyze Word Ratios\n",
    "7. Error Analysis\n",
    "8. Predict Sentiment on Your Own Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac4379",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Data\n",
    "\n",
    "Let's start by importing all the necessary libraries, downloading the required NLTK datasets, and loading the tweet data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25b8f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Download NLTK datasets (run once)\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load positive and negative tweets\n",
    "twitter_samples_path = None  # For local use, can set custom path if needed\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e7d52",
   "metadata": {},
   "source": [
    "## 2. Process the Data\n",
    "\n",
    "Before training our model, we need to clean and preprocess the tweets. We'll use a custom `process_tweet` function to remove noise, punctuation, and stopwords, and to apply stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0550e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Custom tweet preprocessing: lowercase, remove links, handles, punctuation, stopwords, and apply stemming.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove stock tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # Remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # Remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Tokenize\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    clean_tokens = []\n",
    "    for word in tokens:\n",
    "        if (word not in stop_words and  # remove stopwords\n",
    "            word not in string.punctuation and  # remove punctuation\n",
    "            len(word) > 1 and  # remove short tokens\n",
    "            not word.isnumeric()):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            clean_tokens.append(stem_word)\n",
    "    return clean_tokens\n",
    "\n",
    "# Example usage\n",
    "sample_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "print(process_tweet(sample_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7e624",
   "metadata": {},
   "source": [
    "## 3. Implement Helper Functions\n",
    "\n",
    "To train our Naive Bayes model, we need to count how often each word appears in positive and negative tweets. We'll implement two helper functions: `count_tweets` to build the frequency dictionary, and `lookup` to retrieve word-label frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c346f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"
     ]
    }
   ],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    \"\"\"\n",
    "    Count the frequency of each word in positive and negative tweets.\n",
    "    Args:\n",
    "        result: dict to store counts\n",
    "        tweets: list of tweet strings\n",
    "        ys: list/array of labels (0 or 1)\n",
    "    Returns:\n",
    "        result: dict mapping (word, label) to frequency\n",
    "    \"\"\"\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result\n",
    "\n",
    "def lookup(freqs, word, label):\n",
    "    \"\"\"\n",
    "    Return the frequency of a word with a given label from the frequency dictionary.\n",
    "    \"\"\"\n",
    "    return freqs.get((word, label), 0)\n",
    "\n",
    "# Test helper functions\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "print(count_tweets(result, tweets, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62ac7d",
   "metadata": {},
   "source": [
    "## 4. Train Naive Bayes Model\n",
    "\n",
    "Now let's build the frequency dictionary using our helper functions, and implement a custom Naive Bayes training function to compute the logprior and loglikelihood for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94130858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logprior: 0.0\n",
      "Vocabulary size: 8728\n"
     ]
    }
   ],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier: compute logprior and loglikelihood for each word.\n",
    "    \"\"\"\n",
    "    loglikelihood = {}\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    N_pos = N_neg = 0\n",
    "    for (word, label), count in freqs.items():\n",
    "        if label == 1:\n",
    "            N_pos += count\n",
    "        else:\n",
    "            N_neg += count\n",
    "    D = len(train_y)\n",
    "    D_pos = np.sum(train_y)\n",
    "    D_neg = D - D_pos\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    for word in vocab:\n",
    "        freq_pos = freqs.get((word, 1), 0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "    return logprior, loglikelihood\n",
    "\n",
    "# Build frequency dictionary and train model\n",
    "freqs = count_tweets({}, train_x, train_y)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(f\"Logprior: {logprior}\")\n",
    "print(f\"Vocabulary size: {len(loglikelihood)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74609b54",
   "metadata": {},
   "source": [
    "## 5. Test Naive Bayes Model\n",
    "\n",
    "Let's implement a function to predict the sentiment of a tweet using our trained model, and another function to evaluate the model's accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a80276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9945\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a tweet using the trained Naive Bayes model.\n",
    "    \"\"\"\n",
    "    words = process_tweet(tweet)\n",
    "    p = logprior\n",
    "    for word in words:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]\n",
    "    return p\n",
    "\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on the test set.\n",
    "    \"\"\"\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hats.append(1)\n",
    "        else:\n",
    "            y_hats.append(0)\n",
    "    error = np.mean(np.abs(y_hats - test_y))\n",
    "    accuracy = 1 - error\n",
    "    return accuracy\n",
    "\n",
    "# Test prediction and accuracy\n",
    "print(f\"Test accuracy: {test_naive_bayes(test_x, test_y, logprior, loglikelihood):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69566f2",
   "metadata": {},
   "source": [
    "## 6. Analyze Word Ratios\n",
    "\n",
    "Some words are much more likely to appear in positive or negative tweets. Let's implement functions to compute the positive/negative ratio for a word, and to filter words by their sentiment ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e8290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0}, 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0}, ':)': {'positive': 2847, 'negative': 2, 'ratio': 949.3333333333334}, 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0}, ':d': {'positive': 498, 'negative': 0, 'ratio': 499.0}, ':p': {'positive': 104, 'negative': 0, 'ratio': 105.0}, 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0}, ':-)': {'positive': 543, 'negative': 0, 'ratio': 544.0}, \"here'\": {'positive': 20, 'negative': 0, 'ratio': 21.0}, 'youth': {'positive': 14, 'negative': 0, 'ratio': 15.0}, 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0}, 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0}, 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0}, ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0}, 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0}, 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6}, 'via': {'positive': 60, 'negative': 1, 'ratio': 30.5}, 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0}, 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0}, 'fav': {'positive': 11, 'negative': 0, 'ratio': 12.0}, 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0}, 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0}}\n",
      "{':(': {'positive': 1, 'negative': 3663, 'ratio': 0.0005458515283842794}, ':-(': {'positive': 0, 'negative': 378, 'ratio': 0.002638522427440633}, 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05}, '>:(': {'positive': 0, 'negative': 43, 'ratio': 0.022727272727272728}, 'lost': {'positive': 0, 'negative': 19, 'ratio': 0.05}, 'beli̇ev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}, 'wi̇ll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}, 'justi̇n': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}, 'ｓｅｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}, 'ｍｅ': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}\n"
     ]
    }
   ],
   "source": [
    "def get_ratio(freqs, word):\n",
    "    \"\"\"\n",
    "    Compute the positive/negative ratio for a word.\n",
    "    \"\"\"\n",
    "    pos = lookup(freqs, word, 1)\n",
    "    neg = lookup(freqs, word, 0)\n",
    "    ratio = (pos + 1) / (neg + 1)\n",
    "    return {'positive': pos, 'negative': neg, 'ratio': ratio}\n",
    "\n",
    "def get_words_by_threshold(freqs, label, threshold):\n",
    "    \"\"\"\n",
    "    Filter words by their positive/negative ratio.\n",
    "    \"\"\"\n",
    "    word_list = {}\n",
    "    for (word, _), _ in freqs.items():\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "            word_list[word] = pos_neg_ratio\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "            word_list[word] = pos_neg_ratio\n",
    "    return word_list\n",
    "\n",
    "# Example: find strongly positive and negative words\n",
    "print(get_words_by_threshold(freqs, label=1, threshold=10))\n",
    "print(get_words_by_threshold(freqs, label=0, threshold=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68eb11c",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Let's look at some tweets that our model misclassified. This can help us understand the model's limitations and where it might be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0816a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth\tPredicted\tTweet\n",
      "1\t0\t\n",
      "1\t0\ttruli later move know queen bee upward bound movingonup\n",
      "1\t0\tnew report talk burn calori cold work harder warm feel better weather :p\n",
      "1\t0\tharri niall harri born ik stupid wanna chang :d\n",
      "1\t0\t\n",
      "1\t0\t\n",
      "1\t0\tharri niall harri born ik stupid wanna chang :d\n",
      "1\t0\t\n",
      "1\t0\t\n",
      "1\t0\tpark get sunlight\n",
      "1\t0\tuff itna miss karhi thi ap :p\n",
      "0\t1\thello info possibl interest jonatha close join beti :( great\n",
      "1\t0\tpark get sunlight\n",
      "1\t0\tuff itna miss karhi thi ap :p\n",
      "0\t1\thello info possibl interest jonatha close join beti :( great\n",
      "0\t1\tprob fun david\n",
      "0\t1\tprob fun david\n",
      "0\t1\tpat jay\n",
      "0\t1\tpat jay\n"
     ]
    }
   ],
   "source": [
    "print('Truth\\tPredicted\\tTweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print(f'{int(y)}\\t{int(np.sign(y_hat) > 0)}\\t{\" \".join(process_tweet(x))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f32838",
   "metadata": {},
   "source": [
    "## 8. Predict Sentiment on Your Own Tweet\n",
    "\n",
    "Finally, let's use our trained model to predict the sentiment of a custom tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a13f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I am happy because I am learning :)\n",
      "Predicted sentiment score: 9.53\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'I am happy because I am learning :)'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(f\"Tweet: {my_tweet}\")\n",
    "print(f\"Predicted sentiment score: {p:.2f}\")\n",
    "if p > 0:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
