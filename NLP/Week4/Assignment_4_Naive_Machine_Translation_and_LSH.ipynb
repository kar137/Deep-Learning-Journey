{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2d263d",
   "metadata": {},
   "source": [
    "# Assignment 4 - Naive Machine Translation and LSH\n",
    "\n",
    "In this notebook, you'll implement a simple machine translation system (English to French) and explore locality sensitive hashing (LSH) for fast document search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7ba6d",
   "metadata": {},
   "source": [
    "## 1. Word Embeddings Data for English and French Words\n",
    "We'll create small synthetic word embedding dictionaries and English-French mapping dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7fc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6291561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy English and French embeddings (10 words, 8D vectors)\n",
    "words_en = ['cat', 'dog', 'house', 'car', 'tree', 'book', 'apple', 'water', 'sun', 'moon']\n",
    "words_fr = ['chat', 'chien', 'maison', 'voiture', 'arbre', 'livre', 'pomme', 'eau', 'soleil', 'lune']\n",
    "en_embeddings_subset = {w: np.random.randn(8) for w in words_en}\n",
    "fr_embeddings_subset = {w: np.random.randn(8) for w in words_fr}\n",
    "# English to French mapping\n",
    "en_fr_train = dict(zip(words_en, words_fr))\n",
    "en_fr_test = dict(zip(words_en[::-1], words_fr[::-1]))  # reversed for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ccee2",
   "metadata": {},
   "source": [
    "## 1.1 Generate Embedding and Transform Matrices\n",
    "Let's implement the function to get aligned matrices for translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e996872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10, 8) Y_train shape: (10, 8)\n"
     ]
    }
   ],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    X_l, Y_l = [], []\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        if en_word in english_vecs and fr_word in french_vecs:\n",
    "            X_l.append(english_vecs[en_word])\n",
    "            Y_l.append(french_vecs[fr_word])\n",
    "    X = np.vstack(X_l)\n",
    "    Y = np.vstack(Y_l)\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n",
    "X_test, Y_test = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n",
    "print('X_train shape:', X_train.shape, 'Y_train shape:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f97e7",
   "metadata": {},
   "source": [
    "## 2. Translation as Linear Transformation\n",
    "Let's define the loss, gradient, and alignment functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f74c436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 66.0740\n",
      "loss at iteration 25 is: 14.9352\n",
      "loss at iteration 50 is: 7.5385\n",
      "loss at iteration 75 is: 5.2592\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    diff = X @ R - Y\n",
    "    loss = np.sum(diff ** 2) / m\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    grad = (2 / m) * (X.T @ (X @ R - Y))\n",
    "    return grad\n",
    "\n",
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.01, verbose=True):\n",
    "    np.random.seed(42)\n",
    "    R = np.random.randn(X.shape[1], X.shape[1])\n",
    "    for i in range(train_steps):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f'loss at iteration {i} is: {compute_loss(X, Y, R):.4f}')\n",
    "        grad = compute_gradient(X, Y, R)\n",
    "        R -= learning_rate * grad\n",
    "    return R\n",
    "\n",
    "R_train = align_embeddings(X_train, Y_train, train_steps=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9598f23",
   "metadata": {},
   "source": [
    "## 2.2 - Testing the Translation\n",
    "Let's implement nearest neighbor search and test translation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15dfa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.70\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    sims = [cosine_similarity(v, row) for row in candidates]\n",
    "    sorted_ids = np.argsort(sims)[::-1]\n",
    "    return sorted_ids[:k]\n",
    "\n",
    "def test_vocabulary(X, Y, R):\n",
    "    pred = X @ R\n",
    "    num_correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(pred[i], Y, k=1)[0]\n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / X.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "acc = test_vocabulary(X_test, Y_test, R_train)\n",
    "print(f'Accuracy on test set: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97853500",
   "metadata": {},
   "source": [
    "## 3. LSH and Document Search\n",
    "Let's create a dummy tweet dataset and implement LSH for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce9d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy tweet dataset\n",
    "all_tweets = [\n",
    "    'I love cats and dogs',\n",
    "    'The sun is bright',\n",
    "    'Reading a good book',\n",
    "    'Water is life',\n",
    "    'Driving my car to the house',\n",
    "    'The apple is on the tree',\n",
    "    'The moon and the stars',\n",
    "    'My dog drinks water',\n",
    "    'A cat in the house',\n",
    "    'Books are great'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743528fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tweet processor\n",
    "def process_tweet(tweet):\n",
    "    return [w.strip(string.punctuation).lower() for w in tweet.split() if w.strip(string.punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703c4841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vectors shape: (10, 8)\n"
     ]
    }
   ],
   "source": [
    "def get_document_embedding(tweet, en_embeddings):\n",
    "    doc_embedding = np.zeros(8)\n",
    "    for word in process_tweet(tweet):\n",
    "        if word in en_embeddings:\n",
    "            doc_embedding += en_embeddings[word]\n",
    "    return doc_embedding\n",
    "\n",
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    ind2Doc_dict = {}\n",
    "    document_vec_l = []\n",
    "    for i, doc in enumerate(all_docs):\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings)\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "        document_vec_l.append(doc_embedding)\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "    return document_vec_matrix, ind2Doc_dict\n",
    "\n",
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)\n",
    "print('Document vectors shape:', document_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8215967",
   "metadata": {},
   "source": [
    "### LSH: Hashing and Fast Nearest Neighbor Search\n",
    "Let's implement LSH with random planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b20eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VECS = len(all_tweets)\n",
    "N_DIMS = document_vecs.shape[1]\n",
    "N_PLANES = 4\n",
    "N_UNIVERSES = 3\n",
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES)) for _ in range(N_UNIVERSES)]\n",
    "\n",
    "def hash_value_of_vector(v, planes):\n",
    "    dot_product = np.dot(v, planes)\n",
    "    h = (np.sign(dot_product) >= 0).astype(int).flatten()\n",
    "    hash_value = 0\n",
    "    for i in range(len(h)):\n",
    "        hash_value += (2 ** i) * h[i]\n",
    "    return int(hash_value)\n",
    "\n",
    "def make_hash_table(vecs, planes):\n",
    "    num_buckets = 2 ** planes.shape[1]\n",
    "    hash_table = {i: [] for i in range(num_buckets)}\n",
    "    id_table = {i: [] for i in range(num_buckets)}\n",
    "    for i, v in enumerate(vecs):\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "        hash_table[h].append(v)\n",
    "        id_table[h].append(i)\n",
    "    return hash_table, id_table\n",
    "\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2560976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, hash_tables, id_tables, k=1, num_universes_to_use=3):\n",
    "    vecs_to_consider_l = []\n",
    "    ids_to_consider_l = []\n",
    "    ids_to_consider_set = set()\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        hash_table = hash_tables[universe_id]\n",
    "        id_table = id_tables[universe_id]\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "            if doc_id == new_id:\n",
    "                continue\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "    if not vecs_to_consider_l:\n",
    "        return []\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx] for idx in nearest_neighbor_idx_l]\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d19c3e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: I love cats and dogs\n",
      "Similar tweet: Books are great\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_15264\\2437359634.py:2: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n"
     ]
    }
   ],
   "source": [
    "# Example: Find similar tweets\n",
    "doc_id = 0\n",
    "vec_to_search = document_vecs[doc_id]\n",
    "neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, hash_tables, id_tables, k=2, num_universes_to_use=3)\n",
    "print(f'Original tweet: {all_tweets[doc_id]}')\n",
    "for nid in neighbor_ids:\n",
    "    print(f'Similar tweet: {all_tweets[nid]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
