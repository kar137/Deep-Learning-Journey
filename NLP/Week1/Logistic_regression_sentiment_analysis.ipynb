{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304a9112",
   "metadata": {},
   "source": [
    "# Assignment 1: Logistic Regression for Sentiment Analysis\n",
    "\n",
    "Welcome to your first NLP assignment! In this notebook, you will implement logistic regression from scratch to perform sentiment analysis on tweets. You will learn how to extract features from text, train a logistic regression model, make predictions, evaluate performance, and analyze errors.\n",
    "\n",
    "**Outline:**\n",
    "1. Import Required Libraries and Download Data  \n",
    "2. Load and Prepare Tweet Data  \n",
    "3. Create Frequency Dictionary  \n",
    "4. Process and Test Tweet Preprocessing  \n",
    "5. Implement Sigmoid Function  \n",
    "6. Implement Gradient Descent for Logistic Regression  \n",
    "7. Extract Features from Tweets  \n",
    "8. Train the Logistic Regression Model  \n",
    "9. Predict Sentiment for a Tweet  \n",
    "10. Evaluate Model Accuracy on Test Set  \n",
    "11. Perform Error Analysis  \n",
    "12. Predict Sentiment for Custom Tweet  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ea221",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Download Data\n",
    "\n",
    "Let's start by importing the necessary libraries and downloading the required NLTK datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7fddcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from os import getcwd\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# If running locally, you may need to set the nltk data path\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)\n",
    "\n",
    "# Import helper functions from utils.py (assumed to be in the same directory)\n",
    "def process_tweet(tweet):\n",
    "    # Remove RT, URLs, and hashtags (keep the word)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Tokenize\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    # Remove stopwords and punctuation\n",
    "    sw = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in sw and t not in string.punctuation]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, int(y))\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "    return freqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a770d9b",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Tweet Data\n",
    "\n",
    "We will load positive and negative tweets, split them into training and test sets, and create corresponding label arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96bc0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# Create label arrays: 1 for positive, 0 for negative\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"train_y.shape =\", train_y.shape)\n",
    "print(\"test_y.shape =\", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ffaea",
   "metadata": {},
   "source": [
    "## 3. Create Frequency Dictionary\n",
    "\n",
    "We will use the `build_freqs` function to create a frequency dictionary from the training data. This dictionary will map (word, label) pairs to their frequency counts in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93df3779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11396\n"
     ]
    }
   ],
   "source": [
    "# Create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# Check the output\n",
    "print(\"type(freqs) =\", type(freqs))\n",
    "print(\"len(freqs) =\", len(freqs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c69412",
   "metadata": {},
   "source": [
    "## 4. Process and Test Tweet Preprocessing\n",
    "\n",
    "Let's test the `process_tweet` function to see how it tokenizes, removes stopwords, and stems words in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e1f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# Test the function on a sample tweet\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bcce2",
   "metadata": {},
   "source": [
    "## 5. Implement Sigmoid Function\n",
    "\n",
    "Implement the sigmoid activation function, which maps any real value to the (0, 1) interval. Test it on sample inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79936b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n",
      "sigmoid(4.92) = 0.9927537604041685\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C1 GRADED FUNCTION: sigmoid\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    return h\n",
    "\n",
    "# Testing your function \n",
    "print(\"sigmoid(0) =\", sigmoid(0))\n",
    "print(\"sigmoid(4.92) =\", sigmoid(4.92))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d65e9a",
   "metadata": {},
   "source": [
    "## 6. Implement Gradient Descent for Logistic Regression\n",
    "\n",
    "Implement the `gradientDescent` function to optimize the logistic regression weights using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cdb8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.67094970.\n",
      "The resulting vector of weights is [np.float64(4.1e-07), np.float64(0.00035658), np.float64(7.309e-05)]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C2 GRADED FUNCTION: gradientDescent\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    for i in range(0, num_iters):\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "        J = -(1/m) * np.sum(y*np.log(h) + (1-y)*np.log(1-h))\n",
    "        theta = theta - alpha * (1/m) * np.dot(x.T, (h - y))\n",
    "    J = float(J)\n",
    "    return J, theta\n",
    "\n",
    "# Check the function with a synthetic test case\n",
    "np.random.seed(1)\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5b50fe",
   "metadata": {},
   "source": [
    "## 7. Extract Features from Tweets\n",
    "\n",
    "Implement the `extract_features` function to generate feature vectors for tweets based on positive and negative word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed5d819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 3.133e+03 6.100e+01]]\n",
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C3 GRADED FUNCTION: extract_features\n",
    "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros(3) \n",
    "    x[0] = 1 \n",
    "    for word in word_l:\n",
    "        x[1] += freqs.get((word, 1), 0)\n",
    "        x[2] += freqs.get((word, 0), 0)\n",
    "    x = x[None, :]\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n",
    "\n",
    "# Test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)\n",
    "\n",
    "# Test for unknown words\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fb03e",
   "metadata": {},
   "source": [
    "## 8. Train the Logistic Regression Model\n",
    "\n",
    "Stack feature vectors for all training tweets, initialize weights, and train the model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7333e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.22524456.\n",
      "The resulting vector of weights is [np.float64(6e-08), np.float64(0.00053786), np.float64(-0.00055885)]\n"
     ]
    }
   ],
   "source": [
    "# Stack features for all training examples\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "\n",
    "Y = train_y\n",
    "\n",
    "# Train the model\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc6df9",
   "metadata": {},
   "source": [
    "## 9. Predict Sentiment for a Tweet\n",
    "\n",
    "Implement the `predict_tweet` function to predict the sentiment probability for a given tweet using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89fc3856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.519259\n",
      "I am bad -> 0.494338\n",
      "this movie should have been great. -> 0.515962\n",
      "great -> 0.516052\n",
      "great great -> 0.532070\n",
      "great great great -> 0.548023\n",
      "great great great great -> 0.563877\n",
      "[[0.83096623]]\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C4 GRADED FUNCTION: predict_tweet\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    x = extract_features(tweet, freqs)\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    return y_pred\n",
    "\n",
    "# Test predictions on sample tweets\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print(f\"{tweet} -> {predict_tweet(tweet, freqs, theta)[0,0]:.6f}\")\n",
    "\n",
    "# Test on your own tweet\n",
    "my_tweet = 'I am learning :)'\n",
    "print(predict_tweet(my_tweet, freqs, theta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df754f",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model Accuracy on Test Set\n",
    "\n",
    "Implement the `test_logistic_regression` function to compute the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "972f085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9965\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C5 GRADED FUNCTION: test_logistic_regression\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    y_hat = []\n",
    "    for tweet in test_x:\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        if y_pred > 0.5:\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            y_hat.append(0.0)\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = np.squeeze(test_y)\n",
    "    accuracy = np.sum(y_hat == test_y) / len(test_y)\n",
    "    return accuracy\n",
    "\n",
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3f3b4",
   "metadata": {},
   "source": [
    "## 11. Perform Error Analysis\n",
    "\n",
    "Let's analyze tweets that were misclassified by the model and display their processed forms for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb9e05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\tPredicted\tTweet\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48899230\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12588\\863923137.py:7: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
      "1\t0.49632433\tb'park get sunlight'\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48246197\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t0.50983764\tb'u prob fun david'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50040341\tb'pat jay'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000001\tb'belov grandmoth'\n",
      "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
      "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
      "0\t0.50647821\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
     ]
    }
   ],
   "source": [
    "print('Label\\tPredicted\\tTweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa868",
   "metadata": {},
   "source": [
    "## 12. Predict Sentiment for Custom Tweet\n",
    "\n",
    "You can now input your own tweet, process it, predict its sentiment, and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bd0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tweet: ['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
      "Predicted probability: [[0.48122783]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(\"Processed tweet:\", process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(\"Predicted probability:\", y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
