{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c268609c",
   "metadata": {},
   "source": [
    "## Spacy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed31019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9df832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text -- POS\n",
      " ---------\n",
      "I -- PRON\n",
      "bought -- VERB\n",
      "a -- DET\n",
      "pair -- NOUN\n",
      "of -- ADP\n",
      "watch -- NOUN\n",
      "for -- ADP\n",
      "Tom -- PROPN\n",
      "and -- CCONJ\n",
      "Elizabeth -- PROPN\n",
      "which -- PRON\n",
      "costs -- VERB\n",
      "$ -- SYM\n",
      "50 -- NUM\n",
      "each -- PRON\n",
      ". -- PUNCT\n"
     ]
    }
   ],
   "source": [
    "text = \"I bought a pair of watch for Tom and Elizabeth which costs $50 each.\"\n",
    "processed = model(text)\n",
    "\n",
    "\n",
    "print(\"text -- POS\\n ---------\")\n",
    "for token in processed:\n",
    "    print(f\"{token.text} -- {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7ea923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text -- POS -- POS hash\n",
      " ---------\n",
      "I -- PRON -- 95\n",
      "bought -- VERB -- 100\n",
      "a -- DET -- 90\n",
      "pair -- NOUN -- 92\n",
      "of -- ADP -- 85\n",
      "watch -- NOUN -- 92\n",
      "for -- ADP -- 85\n",
      "Tom -- PROPN -- 96\n",
      "and -- CCONJ -- 89\n",
      "Elizabeth -- PROPN -- 96\n",
      "which -- PRON -- 95\n",
      "costs -- VERB -- 100\n",
      "$ -- SYM -- 99\n",
      "50 -- NUM -- 93\n",
      "each -- PRON -- 95\n",
      ". -- PUNCT -- 97\n"
     ]
    }
   ],
   "source": [
    "print(\"text -- POS -- POS hash\\n ---------\")\n",
    "for token in processed:\n",
    "    print(f\"{token.text} -- {token.pos_} -- {token.pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc7d147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP pipelines\n",
    "model.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f4b48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.disable_pipes('parser', 'ner')\n",
    "model.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d4480",
   "metadata": {},
   "source": [
    "## Gensim Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cef6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<23 unique tokens: ['Elizabeth', 'I', 'Tom', 'Tomorrow', 'a']...>\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "document = [\"Tomorrow Tom and Elizabeth are getting married, I need to buy a gift for them.\",\n",
    "            \"I bought a pair of watch for Tom and Elizabeth which costs $50 each.\"]\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokens = [[token for token in docs.split()] for docs in document]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = Dictionary(tokens)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a077af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elizabeth': 0,\n",
       " 'I': 1,\n",
       " 'Tom': 2,\n",
       " 'Tomorrow': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'are': 6,\n",
       " 'buy': 7,\n",
       " 'for': 8,\n",
       " 'getting': 9,\n",
       " 'gift': 10,\n",
       " 'married,': 11,\n",
       " 'need': 12,\n",
       " 'them.': 13,\n",
       " 'to': 14,\n",
       " '$50': 15,\n",
       " 'bought': 16,\n",
       " 'costs': 17,\n",
       " 'each.': 18,\n",
       " 'of': 19,\n",
       " 'pair': 20,\n",
       " 'watch': 21,\n",
       " 'which': 22}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed5be7",
   "metadata": {},
   "source": [
    "## Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c951e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_document = \"I hope they like my gift\"\n",
    "vector = dictionary.doc2bow(new_document.lower().split())\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6d45e",
   "metadata": {},
   "source": [
    "## Model\n",
    "Let's compute the tf-idf of a document and compare it with the its vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3cbb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary : \n",
      "  {'This': 0, 'first': 1, 'is': 2, 'line': 3, 'These': 4, 'are': 5, 'lines': 6, 'second': 7, 'third': 8}\n",
      "\n",
      "\n",
      "Vector of each document: \n",
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(0, 1), (2, 1), (3, 1), (8, 1)]\n",
      "\n",
      "\n",
      "Vector of each document in term of token:\n",
      "[['This', 1], ['first', 1], ['is', 1], ['line', 1]]\n",
      "[['These', 1], ['are', 1], ['lines', 1], ['second', 1]]\n",
      "[['This', 1], ['is', 1], ['line', 1], ['third', 1]]\n",
      "\n",
      "\n",
      "tf-idf assigned to each token:\n",
      "[['This', 0.311], ['first', 0.843], ['is', 0.311], ['line', 0.311]]\n",
      "[['These', 0.5], ['are', 0.5], ['lines', 0.5], ['second', 0.5]]\n",
      "[['This', 0.311], ['is', 0.311], ['line', 0.311], ['third', 0.843]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import numpy\n",
    "\n",
    "\n",
    "documents = ['This is first line',\n",
    "            'These are second lines',\n",
    "            'This is third line']\n",
    "\n",
    "# Token\n",
    "tokens = [[token for token in docs.split()]for docs in documents]\n",
    "\n",
    "# Dictionary\n",
    "dictionary = Dictionary(tokens)\n",
    "print(\"Dictionary : \\n \", dictionary.token2id)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Vector\n",
    "print('Vector of each document: ')\n",
    "vector = [dictionary.doc2bow(token) for token in tokens]\n",
    "for vect in vector:\n",
    "    print(vect)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# BOW of each documents\n",
    "print(\"Vector of each document in term of token:\")\n",
    "for document in vector:\n",
    "    print([[dictionary[id], freq] for id, freq in document])\n",
    "\n",
    "\n",
    "\n",
    "# tfidf model\n",
    "tfidf = models.TfidfModel(vector)\n",
    "\n",
    "\n",
    "# Output of tfidf model\n",
    "print(\"\\n\")\n",
    "print(\"tf-idf assigned to each token:\")\n",
    "for document in tfidf[vector]:\n",
    "    print([[dictionary[id], numpy.around(freq, decimals=3)] for id, freq in document])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e19754",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4387122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg  # corpus reader\n",
    "\n",
    "\n",
    "\n",
    "# download gutenberg corpus\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa670d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_LazyCorpusLoader__args',\n",
       " '_LazyCorpusLoader__kwargs',\n",
       " '_LazyCorpusLoader__load',\n",
       " '_LazyCorpusLoader__name',\n",
       " '_LazyCorpusLoader__reader_cls',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_unload',\n",
       " 'subdir']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all the methods and attributes\n",
    "dir(gutenberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20601066",
   "metadata": {},
   "source": [
    "Few common methods in corpus reader are:\n",
    "\n",
    "- fileids() lists the name of the text documents, i.e, each book in gutenberg.\n",
    "\n",
    "- readme() methods prints the readme file of the corpus.\n",
    "\n",
    "- encoding() gives the encoding type of a text file in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60295167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52fd1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************\n",
      "Raw texts:\n",
      "\n",
      "[The Wisdom of Father Brown by G. K. Che\n",
      "\n",
      "\n",
      "*********************\n",
      "Token of words:\n",
      "\n",
      "['[', 'The', 'Wisdom', 'of', 'Father', 'Brown', 'by', 'G', '.', 'K']\n",
      "\n",
      "\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read raw text\n",
    "print(\"*********************\")\n",
    "raw = gutenberg.raw(\"chesterton-brown.txt\")\n",
    "print(\"Raw texts:\\n\")\n",
    "print(raw[:40])\n",
    "print(\"\\n\")\n",
    "print(\"*********************\")\n",
    "\n",
    "# Read as word tokens\n",
    "words = gutenberg.words(\"chesterton-brown.txt\")\n",
    "print(\"Token of words:\\n\")\n",
    "print(words[:10])  # Print first 10 for readability\n",
    "print(\"\\n\")\n",
    "print(\"*********************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b96bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\udhr.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Abkhaz-Cyrillic+Abkh',\n",
       " 'Abkhaz-UTF8',\n",
       " 'Achehnese-Latin1',\n",
       " 'Achuar-Shiwiar-Latin1',\n",
       " 'Adja-UTF8',\n",
       " 'Afaan_Oromo_Oromiffa-Latin1',\n",
       " 'Afrikaans-Latin1',\n",
       " 'Aguaruna-Latin1',\n",
       " 'Akuapem_Twi-UTF8',\n",
       " 'Albanian_Shqip-Latin1',\n",
       " 'Amahuaca',\n",
       " 'Amahuaca-Latin1',\n",
       " 'Amarakaeri-Latin1',\n",
       " 'Amuesha-Yanesha-UTF8',\n",
       " 'Arabela-Latin1',\n",
       " 'Arabic_Alarabia-Arabic',\n",
       " 'Asante-UTF8',\n",
       " 'Ashaninca-Latin1',\n",
       " 'Asheninca-Latin1',\n",
       " 'Asturian_Bable-Latin1',\n",
       " 'Aymara-Latin1',\n",
       " 'Balinese-Latin1',\n",
       " 'Bambara-UTF8',\n",
       " 'Baoule-UTF8',\n",
       " 'Basque_Euskara-Latin1',\n",
       " 'Batonu_Bariba-UTF8',\n",
       " 'Belorus_Belaruski-Cyrillic',\n",
       " 'Belorus_Belaruski-UTF8',\n",
       " 'Bemba-Latin1',\n",
       " 'Bengali-UTF8',\n",
       " 'Beti-UTF8',\n",
       " 'Bichelamar-Latin1',\n",
       " 'Bikol_Bicolano-Latin1',\n",
       " 'Bora-Latin1',\n",
       " 'Bosnian_Bosanski-Cyrillic',\n",
       " 'Bosnian_Bosanski-Latin2',\n",
       " 'Bosnian_Bosanski-UTF8',\n",
       " 'Breton-Latin1',\n",
       " 'Bugisnese-Latin1',\n",
       " 'Bulgarian_Balgarski-Cyrillic',\n",
       " 'Bulgarian_Balgarski-UTF8',\n",
       " 'Cakchiquel-Latin1',\n",
       " 'Campa_Pajonalino-Latin1',\n",
       " 'Candoshi-Shapra-Latin1',\n",
       " 'Caquinte-Latin1',\n",
       " 'Cashibo-Cacataibo-Latin1',\n",
       " 'Cashinahua-Latin1',\n",
       " 'Catalan-Latin1',\n",
       " 'Catalan_Catala-Latin1',\n",
       " 'Cebuano-Latin1',\n",
       " 'Chamorro-Latin1',\n",
       " 'Chayahuita-Latin1',\n",
       " 'Chechewa_Nyanja-Latin1',\n",
       " 'Chickasaw-Latin1',\n",
       " 'Chinanteco-Ajitlan-Latin1',\n",
       " 'Chinanteco-UTF8',\n",
       " 'Chinese_Mandarin-GB2312',\n",
       " 'Chuuk_Trukese-Latin1',\n",
       " 'Cokwe-Latin1',\n",
       " 'Corsican-Latin1',\n",
       " 'Croatian_Hrvatski-Latin2',\n",
       " 'Czech-Latin2',\n",
       " 'Czech-UTF8',\n",
       " 'Czech_Cesky-Latin2',\n",
       " 'Czech_Cesky-UTF8',\n",
       " 'Dagaare-UTF8',\n",
       " 'Dagbani-UTF8',\n",
       " 'Dangme-UTF8',\n",
       " 'Danish_Dansk-Latin1',\n",
       " 'Dendi-UTF8',\n",
       " 'Ditammari-UTF8',\n",
       " 'Dutch_Nederlands-Latin1',\n",
       " 'Edo-Latin1',\n",
       " 'English-Latin1',\n",
       " 'Esperanto-UTF8',\n",
       " 'Estonian_Eesti-Latin1',\n",
       " 'Ewe_Eve-UTF8',\n",
       " 'Fante-UTF8',\n",
       " 'Faroese-Latin1',\n",
       " 'Farsi_Persian-UTF8',\n",
       " 'Farsi_Persian-v2-UTF8',\n",
       " 'Fijian-Latin1',\n",
       " 'Filipino_Tagalog-Latin1',\n",
       " 'Finnish_Suomi-Latin1',\n",
       " 'Fon-UTF8',\n",
       " 'French_Francais-Latin1',\n",
       " 'Frisian-Latin1',\n",
       " 'Friulian_Friulano-Latin1',\n",
       " 'Ga-UTF8',\n",
       " 'Gagauz_Gagauzi-UTF8',\n",
       " 'Galician_Galego-Latin1',\n",
       " 'Garifuna_Garifuna-Latin1',\n",
       " 'German_Deutsch-Latin1',\n",
       " 'Gonja-UTF8',\n",
       " 'Greek_Ellinika-Greek',\n",
       " 'Greek_Ellinika-UTF8',\n",
       " 'Greenlandic_Inuktikut-Latin1',\n",
       " 'Guarani-Latin1',\n",
       " 'Guen_Mina-UTF8',\n",
       " 'HaitianCreole_Kreyol-Latin1',\n",
       " 'HaitianCreole_Popular-Latin1',\n",
       " 'Hani-Latin1',\n",
       " 'Hausa_Haoussa-Latin1',\n",
       " 'Hawaiian-UTF8',\n",
       " 'Hebrew_Ivrit-Hebrew',\n",
       " 'Hebrew_Ivrit-UTF8',\n",
       " 'Hiligaynon-Latin1',\n",
       " 'Hindi-UTF8',\n",
       " 'Hindi_web-UTF8',\n",
       " 'Hmong_Miao-Sichuan-Guizhou-Yunnan-Latin1',\n",
       " 'Hmong_Miao-SouthernEast-Guizhou-Latin1',\n",
       " 'Hmong_Miao_Northern-East-Guizhou-Latin1',\n",
       " 'Hrvatski_Croatian-Latin2',\n",
       " 'Huasteco-Latin1',\n",
       " 'Huitoto_Murui-Latin1',\n",
       " 'Hungarian_Magyar-Latin1',\n",
       " 'Hungarian_Magyar-Latin2',\n",
       " 'Hungarian_Magyar-UTF8',\n",
       " 'Ibibio_Efik-Latin1',\n",
       " 'Icelandic_Yslenska-Latin1',\n",
       " 'Ido-Latin1',\n",
       " 'Igbo-UTF8',\n",
       " 'Iloko_Ilocano-Latin1',\n",
       " 'Indonesian-Latin1',\n",
       " 'Interlingua-Latin1',\n",
       " 'Inuktikut_Greenlandic-Latin1',\n",
       " 'IrishGaelic_Gaeilge-Latin1',\n",
       " 'Italian-Latin1',\n",
       " 'Italian_Italiano-Latin1',\n",
       " 'Japanese_Nihongo-EUC',\n",
       " 'Japanese_Nihongo-SJIS',\n",
       " 'Japanese_Nihongo-UTF8',\n",
       " 'Javanese-Latin1',\n",
       " 'Jola-Fogny_Diola-UTF8',\n",
       " 'Kabye-UTF8',\n",
       " 'Kannada-UTF8',\n",
       " 'Kaonde-Latin1',\n",
       " 'Kapampangan-Latin1',\n",
       " 'Kasem-UTF8',\n",
       " 'Kazakh-Cyrillic',\n",
       " 'Kazakh-UTF8',\n",
       " 'Kiche_Quiche-Latin1',\n",
       " 'Kicongo-Latin1',\n",
       " 'Kimbundu_Mbundu-Latin1',\n",
       " 'Kinyamwezi_Nyamwezi-Latin1',\n",
       " 'Kinyarwanda-Latin1',\n",
       " 'Kituba-Latin1',\n",
       " 'Korean_Hankuko-UTF8',\n",
       " 'Kpelewo-UTF8',\n",
       " 'Krio-UTF8',\n",
       " 'Kurdish-UTF8',\n",
       " 'Lamnso_Lam-nso-UTF8',\n",
       " 'Latin_Latina-Latin1',\n",
       " 'Latin_Latina-v2-Latin1',\n",
       " 'Latvian-Latin1',\n",
       " 'Limba-UTF8',\n",
       " 'Lingala-Latin1',\n",
       " 'Lithuanian_Lietuviskai-Baltic',\n",
       " 'Lozi-Latin1',\n",
       " 'Luba-Kasai_Tshiluba-Latin1',\n",
       " 'Luganda_Ganda-Latin1',\n",
       " 'Lunda_Chokwe-lunda-Latin1',\n",
       " 'Luvale-Latin1',\n",
       " 'Luxembourgish_Letzebuergeusch-Latin1',\n",
       " 'Macedonian-UTF8',\n",
       " 'Madurese-Latin1',\n",
       " 'Makonde-Latin1',\n",
       " 'Malagasy-Latin1',\n",
       " 'Malay_BahasaMelayu-Latin1',\n",
       " 'Maltese-UTF8',\n",
       " 'Mam-Latin1',\n",
       " 'Maninka-UTF8',\n",
       " 'Maori-Latin1',\n",
       " 'Mapudungun_Mapuzgun-Latin1',\n",
       " 'Mapudungun_Mapuzgun-UTF8',\n",
       " 'Marshallese-Latin1',\n",
       " 'Matses-Latin1',\n",
       " 'Mayan_Yucateco-Latin1',\n",
       " 'Mazahua_Jnatrjo-UTF8',\n",
       " 'Mazateco-Latin1',\n",
       " 'Mende-UTF8',\n",
       " 'Mikmaq_Micmac-Mikmaq-Latin1',\n",
       " 'Minangkabau-Latin1',\n",
       " 'Miskito_Miskito-Latin1',\n",
       " 'Mixteco-Latin1',\n",
       " 'Mongolian_Khalkha-Cyrillic',\n",
       " 'Mongolian_Khalkha-UTF8',\n",
       " 'Moore_More-UTF8',\n",
       " 'Nahuatl-Latin1',\n",
       " 'Ndebele-Latin1',\n",
       " 'Nepali-UTF8',\n",
       " 'Ngangela_Nyemba-Latin1',\n",
       " 'NigerianPidginEnglish-Latin1',\n",
       " 'Nomatsiguenga-Latin1',\n",
       " 'NorthernSotho_Pedi-Sepedi-Latin1',\n",
       " 'Norwegian-Latin1',\n",
       " 'Norwegian_Norsk-Bokmal-Latin1',\n",
       " 'Norwegian_Norsk-Nynorsk-Latin1',\n",
       " 'Nyanja_Chechewa-Latin1',\n",
       " 'Nyanja_Chinyanja-Latin1',\n",
       " 'Nzema-UTF8',\n",
       " 'OccitanAuvergnat-Latin1',\n",
       " 'OccitanLanguedocien-Latin1',\n",
       " 'Oromiffa_AfaanOromo-Latin1',\n",
       " 'Osetin_Ossetian-UTF8',\n",
       " 'Oshiwambo_Ndonga-Latin1',\n",
       " 'Otomi_Nahnu-Latin1',\n",
       " 'Paez-Latin1',\n",
       " 'Palauan-Latin1',\n",
       " 'Peuhl-UTF8',\n",
       " 'Picard-Latin1',\n",
       " 'Pipil-Latin1',\n",
       " 'Polish-Latin2',\n",
       " 'Polish_Polski-Latin2',\n",
       " 'Ponapean-Latin1',\n",
       " 'Portuguese_Portugues-Latin1',\n",
       " 'Pulaar-UTF8',\n",
       " 'Punjabi_Panjabi-UTF8',\n",
       " 'Purhepecha-UTF8',\n",
       " 'Qechi_Kekchi-Latin1',\n",
       " 'Quechua-Latin1',\n",
       " 'Quichua-Latin1',\n",
       " 'Rarotongan_MaoriCookIslands-Latin1',\n",
       " 'Rhaeto-Romance_Rumantsch-Latin1',\n",
       " 'Romani-Latin1',\n",
       " 'Romani-UTF8',\n",
       " 'Romanian-Latin2',\n",
       " 'Romanian_Romana-Latin2',\n",
       " 'Rukonzo_Konjo-Latin1',\n",
       " 'Rundi_Kirundi-Latin1',\n",
       " 'Runyankore-rukiga_Nkore-kiga-Latin1',\n",
       " 'Russian-Cyrillic',\n",
       " 'Russian-UTF8',\n",
       " 'Russian_Russky-Cyrillic',\n",
       " 'Russian_Russky-UTF8',\n",
       " 'Sami_Lappish-UTF8',\n",
       " 'Sammarinese-Latin1',\n",
       " 'Samoan-Latin1',\n",
       " 'Sango_Sangho-Latin1',\n",
       " 'Sanskrit-UTF8',\n",
       " 'Saraiki-UTF8',\n",
       " 'Sardinian-Latin1',\n",
       " 'ScottishGaelic_GaidhligAlbanach-Latin1',\n",
       " 'Seereer-UTF8',\n",
       " 'Serbian_Srpski-Cyrillic',\n",
       " 'Serbian_Srpski-Latin2',\n",
       " 'Serbian_Srpski-UTF8',\n",
       " 'Sharanahua-Latin1',\n",
       " 'Shipibo-Conibo-Latin1',\n",
       " 'Shona-Latin1',\n",
       " 'Sinhala-UTF8',\n",
       " 'Siswati-Latin1',\n",
       " 'Slovak-Latin2',\n",
       " 'Slovak_Slovencina-Latin2',\n",
       " 'Slovenian_Slovenscina-Latin2',\n",
       " 'SolomonsPidgin_Pijin-Latin1',\n",
       " 'Somali-Latin1',\n",
       " 'Soninke_Soninkanxaane-UTF8',\n",
       " 'Sorbian-Latin2',\n",
       " 'SouthernSotho_Sotho-Sesotho-Sutu-Sesutu-Latin1',\n",
       " 'Spanish-Latin1',\n",
       " 'Spanish_Espanol-Latin1',\n",
       " 'Sukuma-Latin1',\n",
       " 'Sundanese-Latin1',\n",
       " 'Sussu_Soussou-Sosso-Soso-Susu-UTF8',\n",
       " 'Swaheli-Latin1',\n",
       " 'Swahili_Kiswahili-Latin1',\n",
       " 'Swedish_Svenska-Latin1',\n",
       " 'Tahitian-UTF8',\n",
       " 'Tenek_Huasteco-Latin1',\n",
       " 'Tetum-Latin1',\n",
       " 'Themne_Temne-UTF8',\n",
       " 'Tiv-Latin1',\n",
       " 'Toba-UTF8',\n",
       " 'Tojol-abal-Latin1',\n",
       " 'TokPisin-Latin1',\n",
       " 'Tonga-Latin1',\n",
       " 'Tongan_Tonga-Latin1',\n",
       " 'Totonaco-Latin1',\n",
       " 'Trukese_Chuuk-Latin1',\n",
       " 'Turkish_Turkce-Turkish',\n",
       " 'Turkish_Turkce-UTF8',\n",
       " 'Tzeltal-Latin1',\n",
       " 'Tzotzil-Latin1',\n",
       " 'Uighur_Uyghur-Latin1',\n",
       " 'Uighur_Uyghur-UTF8',\n",
       " 'Ukrainian-Cyrillic',\n",
       " 'Ukrainian-UTF8',\n",
       " 'Umbundu-Latin1',\n",
       " 'Urarina-Latin1',\n",
       " 'Uzbek-Latin1',\n",
       " 'Vietnamese-ALRN-UTF8',\n",
       " 'Vietnamese-UTF8',\n",
       " 'Vlach-Latin1',\n",
       " 'Walloon_Wallon-Latin1',\n",
       " 'Wama-UTF8',\n",
       " 'Waray-Latin1',\n",
       " 'Wayuu-Latin1',\n",
       " 'Welsh_Cymraeg-Latin1',\n",
       " 'WesternSotho_Tswana-Setswana-Latin1',\n",
       " 'Wolof-Latin1',\n",
       " 'Xhosa-Latin1',\n",
       " 'Yagua-Latin1',\n",
       " 'Yao-Latin1',\n",
       " 'Yapese-Latin1',\n",
       " 'Yoruba-UTF8',\n",
       " 'Zapoteco-Latin1',\n",
       " 'Zapoteco-SanLucasQuiavini-Latin1',\n",
       " 'Zhuang-Latin1',\n",
       " 'Zulu-Latin1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import udhr\n",
    "nltk.download('udhr')\n",
    "\n",
    "\n",
    "# Field ids:\n",
    "udhr.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4cfea51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                     मानव अधिकारको विश्वव्यापी घोषणा \\n\\nप्रस्तावना \\n       मानव परिवारका सबै सदस्यहरूको अन्तर्निहित मान तथा सम्मान र अवछिन्न अधिकारहरूको मान्यता नै स्वतन्त्रता, न्याय, र शान्तिको आधार भएकोले,\\n \\n       मानव अधिकारहरू प्रति अवहेलना तथा अनादरको परिणामबाटै नै काम भड मानव जातिको अन्त स्करणमा चिट पुर्\\u200dयाइएको हुनाले र मानवहरूले धर्म र वाक स्वन्त्रता तथा भए र अभावबाट मुक्ति पाउनु पर्छ भन्ने सर्व साधारण जनताको घोषित  आकांक्षा भएकोले,\\n\\nअत्याचार र दमनको विरुद्ध अरू उपाय नपाएर विद्रोह गर्नू नै अन्तिम उपाय हो भन्ने नउठानुन  हो भने मानव अधिकारहरू कानुनी शासनद्वारा संरक्षित रहनु अति आवश्यक भएकोले, \\n\\n      राष्ट्रहरूका बीच मैत्री सम्बन्ध वृद्धि गर्न आवश्यक भएकोले \\n\\nसंयुक्त राष्ट्र संघका जनता हरूले मानवका मौलिक अधिकारहरू र मनुष्यको मान तथा कदर र नर -नारीहरूको सम्मान अधिकारहरू प्रति पुनः विश्वासको पुस्ट्याइँ अधिकार पत्रमा गरि बढि स्वतन्त्रताको आधारमा सामाजिक प्रगति एवं जीवनको\\n\\n      सदस्य राष्ट्रहरू र ती राष्ट्रहरूका अधिकारमा रहेका प्रादेशिक जनताहरूमा समेत ती अधिकारहरू र स्वतन्त्र हरू को विश्वव्यापी र प्रभावशाली मान्यता प्राप्त गर्नको लागि प्रत्येक व्यक्ति र समाजको अंगले निरन्तर मनमा राखि राष्ट्रिय र अन्तरराष्ट्रिय प्रगतिशील कदमहरू द्वारा यी अधिकार र सवतन्त्रताहरू प्रति दीक्षा तथा शिक्षाद्वारा श्रद्धा बढाउन प्रयत्न गरून् भनि साधारण सभाले मानव अधिकारको यस विश्वव्यापी घोषणालाई  सबै जनता र राष्ट्रको निम्ति सजिया मापदण्ड भनि घोषणा गर्दछ \\n\\nधारा १ सबै व्यक्ति हरू जन्मजात स्वतन्त्र हुन ती सबैको समान अधिकार र महत्व छ। निजहरूमा विचार शक्ति र सद्धिचार भएकोले निजहरूले आपसमा भातृत्वको भावना बाट व्यवहार गर्नु पर्छ।\\n\\nधारा २ जाति, वर्ण, लिङ्ग, भाषा, धर्म राजनैतिक वा हरू विचार राष्ट्रिय वा सामाजिक उत्पति सम्पत्ति वा अरू कुनै मर्यादाको आधारमा भेदभाव नगरि प्रत्येक व्यक्तिलाई यस घोषणामा उल्लेखित अधिकार र स्वतन्त्रता को अधिकार हुनेछ। यसको अतिरिक्त चाहे कुनै देश सवतन्त्र होस वा संरक्षित, स्वशासनरहित वा परिमित प्रभुशत्ता भएको होस त्यहाँका व्यक्तिहरूमा राजनैतिक क्षेत्रीय वा अन्तराष्ट्रिय  भेदभाव गरिने छैन।\\n\\nधारा ३ प्रत्येक व्यक्तिलाई व्यक्तिक स्वतन्त्रता र आत्मरक्षाको अधिकार हुनेछ।\\n\\nधारा ४ बाँधा वा दास बनाइ कसैलाई पनि राखिने छैन। दासत्व र दास दासीको व्यापार प्रत्येक रूपमा निषेध गरिएको छ।\\n\\nधारा ५ कुनै व्यक्तिलाई पनि शारीरिक यातना दिइने छैन अथवा निर्दयी अमानुषिक वा अपमानजनक व्यवहार वा सजाय गरिने छैन।\\n\\nधारा ८ संविधान वा कानुन द्वारा दिइएको मौलिक अधिकारहरू भङ्ग गर्ने कार्यहरूको विरुद्ध योग्य राष्ट्रिय अदालतको प्रभावोत्पादक उपचारको सबैलाई अधिकार हुनेछ।\\n\\nधारा ९ कसैलाई पनि मनमानी ढाँगले देश निष्कासित वा गिरफ्तार वा नजर बन्द गरिने छैन।\\n\\nधारा १० कुनै व्यक्ति उपर लगाइएको फौजदारी आरोपको विरुद्ध निजको अधिकार दायित्व निरोपन  गर्दै स्वतन्त्र र न्याय युक्त  अदालतबाट सुनुवाइको हक प्रत्येक व्यक्ति लाइ समान रूपले हुनेछ। \\n\\nधारा ११ (१) दण्डनीय अपराधको आरोप लागेको प्रत्येक सुविधाहरू दिएको बचाउको सबै आवश्यक सुविधाहरू दिएको खुल्ला अदालतले  कानुन अनुसार निजलाई अपराधी नठहराउन्जेल निज निरपराध मानिने छ।\\n\\n          (२) कुनै व्यक्तिले गरे वा नगरेको कुनै काम उक्त समयमा राष्ट्रिय वा अन्तरराष्ट्रिय कानुन अन्तर्गत अपराध मानिँदैन भने त्यस्तो कामको प्रति कुनै पनि व्यक्तिलाई दोषी ठहराइने छैन कुनै पनि व्यक्तिलाई   अपराध गर्दा उक्त समयमा दिन सकिने सजाय भन्दा बढि सजाय दिइने छैन। \\n\\nधारा १२ कुनैपनि व्यक्तिको गोप्यता परिवार, घर वा पत्र व्यवहारको प्रति मनमानी हस्तक्षेप गरिने छैन र कसैको सम्मान तथा ख्याति माथि चोट पुर्\\u200dयाइने छैन र त्यस्तो हस्तक्षेप वा चोटको विरुद्ध प्रत्येक व्यक्तिलाई कानुनी संरक्षणको अधिकार हुनेछ।\\n\\nधरा १३ (१) प्रत्येक व्यक्तिलाई प्रत्येक देशको सिमाना भित्र स्वतन्त्रता पूर्वक विचरण र बसोबास गर्ने अधिकार हुनेछ।  \\n\\n           (२) संयुक्त राष्ट्र सङ्घको उद्देश्य र सिद्धान्तको विपरीत कार्यहरू वा अराजनैतिक अपराधहरूको अभियोगको सम्बन्धमा यो अधिकार प्रयोग गर्न सकिने छैन।\\n\\nधारा १५ (१) प्रत्येक व्यक्तिलाई राष्ट्रको नागरिकताको अधिकार छ।\\n\\n             (२) कुनैपनि व्यक्तिलाई मनमानी ढङ्गले निजको नागरिकता बाट बन्चित गरिने छैन र नागरिकता परिवर्तन गर्ने अधिकारलाई इन्कार गरिने छैन।\\n   \\nधारा १६ (१) जाति राष्ट्र'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr.raw('Nepali-UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea537baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Download stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#### Note: stopwords is a corpus reader and 'stopwords' is a corpus. ###\n",
    "\n",
    "# stopwords from english language\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Print stopwords\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadcf15e",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753ab532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tomorrow', 'Tom', 'and', 'Elizabeth', 'are', 'getting', 'married', ',', 'I', 'need', 'to', 'buy', 'a', 'gift', 'for', 'them', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "text = \"Tomorrow Tom and Elizabeth are getting married, I need to buy a gift for them.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ad9a841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On my God!', 'I almost fall down.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text = \"On my God! I almost fall down.\"\n",
    "tokens = sent_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d55d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bought', 'a', 'pair', 'of', 'watch', 'for', 'Tom', 'and', 'Elizabeth', 'which', 'costs', '50', 'each']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"I bought a pair of watch for Tom and Elizabeth which costs $50 each.\"\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z0-9]+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a558b",
   "metadata": {},
   "source": [
    "### Frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a87d9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 627),\n",
       " ('the', 579),\n",
       " ('i', 533),\n",
       " ('to', 446),\n",
       " ('you', 391),\n",
       " ('of', 354),\n",
       " ('that', 289),\n",
       " ('a', 267),\n",
       " ('not', 257),\n",
       " ('is', 253)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = gutenberg.words('shakespeare-caesar.txt')\n",
    "freq_dist = nltk.FreqDist(word.lower() for word in words if word.isalpha())\n",
    "freq_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f88a1",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0beb5f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode: क\n",
      "After encoding: b'\\xe0\\xa4\\x95'\n",
      "After decoding: क\n"
     ]
    }
   ],
   "source": [
    "# Unicode\n",
    "unicode = '\\u0915'\n",
    "print(\"Unicode:\", unicode)\n",
    "encoded = unicode.encode('utf8')\n",
    "print(\"After encoding:\",encoded)\n",
    "print(\"After decoding:\",encoded.decode('utf8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
