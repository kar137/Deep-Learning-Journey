{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c18a94",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79057eb",
   "metadata": {},
   "source": [
    "## Sequential Model\n",
    "Sequential model is a linear stack of layers but you are limited to one input and output. Sequential model is best choice when you want simple stack by stack layer model. The first layer of sequential model need to know input shape of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8804e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=3, bias=True)\n",
      "  (5): Softmax(dim=-1)\n",
      ")\n",
      "tensor([[0.2808, 0.4005, 0.3187],\n",
      "        [0.3185, 0.3433, 0.3382],\n",
      "        [0.2809, 0.4010, 0.3181],\n",
      "        [0.3347, 0.3279, 0.3374],\n",
      "        [0.2722, 0.4062, 0.3216]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(4, 5),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(5, 8),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(8, 3),\n",
    "  nn.Softmax(dim=-1),\n",
    ")\n",
    "# Model Architecture\n",
    "print(model)\n",
    "# forward pass with random input\n",
    "output = model(torch.randn(5, 4))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0df3ac",
   "metadata": {},
   "source": [
    "## Manual building parameters and operating using torch.nn.functional API\n",
    "The torch.nn.functional API delivers a more low-level and flexible way to build models than the sequential model. It can handle complex models, multiple input or output models, custom operations, as well as models that share layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4346f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         8.7662e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# generate random input, 4 examples with 28*28 features each are created\n",
    "inputs = torch.randn(4, 28*28)\n",
    "\n",
    "# generate parameters(Weights and Biases) for each layers,\n",
    "# gradient calculation for this parameters need to be enabled for back propagation\n",
    "# define the weights, shape should be (out_dim, in_dim)\n",
    "W1 = torch.randn((64, 28*28), requires_grad=True)\n",
    "W2 = torch.randn((64, 64), requires_grad=True)\n",
    "W3 = torch.randn((10, 64), requires_grad=True)\n",
    "\n",
    "# define the bias terms, shape should be (out_dim)\n",
    "B1 = torch.randn((64), requires_grad=True)\n",
    "B2 = torch.randn((64), requires_grad=True)\n",
    "B3 = torch.randn((10), requires_grad=True)\n",
    "\n",
    "# Layer 1 operation\n",
    "h1 = F.relu(F.linear(inputs, W1, B1))\n",
    "# Layer 2 operation\n",
    "h2 = F.relu(F.linear(h1, W2, B2))\n",
    "# Output Layer operation\n",
    "output = F.softmax(F.linear(h2, W3, B3), dim=-1)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543d43e",
   "metadata": {},
   "source": [
    "## Model Subclassing\n",
    "Model subclassing is completely customizable and allows us to build our own custom forward-pass of the model. In PyTorch, the nn.Module class is the root class used to define a model architecture. In model subclassing implementation, MyModel inherits from the nn.Module class. The structure of model subclassing is that we create layers in the initializer __init__() and define the forward pass in the forward() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c4bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (dense1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (dense2): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (dense3): Linear(in_features=32, out_features=5, bias=True)\n",
      "  (act3): Softmax(dim=-1)\n",
      ")\n",
      "tensor([[0.1520, 0.1910, 0.1675, 0.2593, 0.2302],\n",
      "        [0.1750, 0.2015, 0.1694, 0.2297, 0.2243],\n",
      "        [0.1388, 0.2035, 0.1608, 0.2643, 0.2326],\n",
      "        [0.1638, 0.2022, 0.1655, 0.2385, 0.2300],\n",
      "        [0.1533, 0.2049, 0.1726, 0.2422, 0.2270],\n",
      "        [0.1462, 0.2080, 0.1634, 0.2507, 0.2317],\n",
      "        [0.1659, 0.2094, 0.1630, 0.2315, 0.2301],\n",
      "        [0.1485, 0.2055, 0.1657, 0.2484, 0.2318]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# MyModel inherits from the nn.Module class\n",
    "class MyModel(nn.Module):\n",
    "  #Initialize the layers\n",
    "  def __init__(self, in_features, **kwargs):\n",
    "    super(MyModel,self).__init__(**kwargs)\n",
    "    self.dense1 = nn.Linear(in_features, 16)\n",
    "    self.act1 = nn.ReLU()\n",
    "    self.dense2 = nn.Linear(16, 32)\n",
    "    self.act2 = nn.ReLU()\n",
    "    self.dense3 = nn.Linear(32, 5)\n",
    "    self.act3 = nn.Softmax(dim=-1)\n",
    "\n",
    "  # Forward pass\n",
    "  def forward(self,inputs):\n",
    "    x = self.act1(self.dense1(inputs))\n",
    "    x = self.act2(self.dense2(x))\n",
    "    x = self.act3(self.dense3(x))\n",
    "    return x\n",
    "# Model initialization and architecture\n",
    "model = MyModel(4)\n",
    "print(model)\n",
    "# Forward pass with random input\n",
    "output = model(torch.randn(8, 4))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f815d69",
   "metadata": {},
   "source": [
    "## Multi-input and Multi-output Models\n",
    "PyTorch model subclassing allows for flexible architectures, including multiple inputs and outputs. Below are examples for single/multi input/output scenarios and shared layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0384afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleInpMultiOutModel(\n",
      "  (dense): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (out1): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (out2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[0.2984, 0.4038, 0.2978],\n",
      "        [0.3555, 0.3255, 0.3190],\n",
      "        [0.3401, 0.3732, 0.2867],\n",
      "        [0.3355, 0.3006, 0.3639]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.4712],\n",
      "        [0.4777],\n",
      "        [0.4711],\n",
      "        [0.4636]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class SingleInpMultiOutModel(nn.Module):\n",
    "    def __init__(self, in_features, **kwargs):\n",
    "        super(SingleInpMultiOutModel, self).__init__(**kwargs)\n",
    "        # Common Layer\n",
    "        self.dense = nn.Linear(in_features, 128)\n",
    "\n",
    "        # Output layers\n",
    "        self.out1 = nn.Linear(128, 3)\n",
    "        self.out2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        comm = F.relu(self.dense(inputs))\n",
    "        out1 = F.softmax(self.out1(comm), dim=-1)\n",
    "        out2 = torch.sigmoid(self.out2(comm))\n",
    "        return out1, out2\n",
    "\n",
    "# Define Model\n",
    "model = SingleInpMultiOutModel(64)\n",
    "print(model)\n",
    "# Forward pass with random input\n",
    "out1, out2 = model(torch.randn(4, 64))\n",
    "print(out1)\n",
    "print(out2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
