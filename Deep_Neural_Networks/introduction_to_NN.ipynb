{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55d5171",
   "metadata": {},
   "source": [
    "# Neural Networks – Learning Notes\n",
    "\n",
    "## Biological Neurons\n",
    "\n",
    "**Human Brain Neurons:**  \n",
    "- ~10¹¹ neurons, each connected to ~10⁴ others.  \n",
    "- Neuron switching time: ~0.001 sec; computer switching time: ~10⁻¹⁰ sec.  \n",
    "- Parallel processing allows rapid recognition and decision-making.  \n",
    "\n",
    "**Neuron Structure & Terminologies:**  \n",
    "\n",
    "| Component | Function |\n",
    "|-----------|---------|\n",
    "| **Dendrites** | Collect information from neighboring neurons and transmit to axon. |\n",
    "| **Cell Body (Soma)** | Processes inputs from dendrites; contains organelles (mitochondria, Golgi bodies, etc.). |\n",
    "| **Axon** | Transfers impulses from soma to dendrites of next neuron via synapse. Connected to soma via **axon hillock**. |\n",
    "| **Synapse** | Chemical bridge between axon terminal and next neuron's dendrites. |\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Network Concept\n",
    "\n",
    "- A neuron: **dendrites** collect input → **axon** sends output → **synapse** transmits signal.  \n",
    "- **Mathematical Model:**  \n",
    "  - Input vector → output vector.  \n",
    "  - Synapses modeled as **weights** to carry information.  \n",
    "- Multiple neurons → **Artificial Neural Network (ANN)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## Artificial Neuron Models\n",
    "\n",
    "### Linear Neuron\n",
    "- Simple, computationally limited.  \n",
    "- Similar to linear regression.  \n",
    "\n",
    "**Mathematical Formulation:**  \n",
    "- Scalar form:  \n",
    "\\[\n",
    "y = b + \\sum_{i=1}^{n} x_i w_i\n",
    "\\]  \n",
    "- Vector form:  \n",
    "\\[\n",
    "y = \\mathbf{w} \\cdot \\mathbf{x} = \\mathbf{w}^\\top \\mathbf{x}\n",
    "\\]  \n",
    "\n",
    "Where:  \n",
    "- \\(y\\) = target/output  \n",
    "- \\(b\\) = bias term  \n",
    "- \\(x_i\\) = ith input  \n",
    "- \\(w_i\\) = ith weight  \n",
    "\n",
    "---\n",
    "\n",
    "### Perceptron (Rosenblatt, 1958)\n",
    "- A generalization of linear neurons, also called **Linear Threshold Unit (LTU)**.  \n",
    "- Works on non-Boolean values with weighted inputs.  \n",
    "- Used for **linearly separable data**.  \n",
    "\n",
    "**Perceptron Components:**  \n",
    "1. **Inputs:** Vector of features \\(x = [x_1, x_2, ..., x_n]^T\\)  \n",
    "2. **Weights:** \\(w = [w_1, w_2, ..., w_n]^T\\)  \n",
    "3. **Weighted Sum:**  \n",
    "\\[\n",
    "\\hat{y} = \\sum_{i=1}^{n} w_i x_i\n",
    "\\]  \n",
    "4. **Bias (Intercept):**  \n",
    "\\[\n",
    "\\hat{y} = b + \\sum_{i=1}^{n} w_i x_i\n",
    "\\]  \n",
    "5. **Step Activation Function:**  \n",
    "\\[\n",
    "output =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\hat{y} \\ge \\text{threshold} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\]  \n",
    "- Bias can act as the threshold: \\(threshold = -bias\\)  \n",
    "\n",
    "**Intuition:**  \n",
    "- Large bias → neuron activates easily  \n",
    "- Small bias → neuron rarely fires  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05b8d6",
   "metadata": {},
   "source": [
    "# Computational Graph and Backpropagation – Learning Notes\n",
    "\n",
    "## Computational Graphs\n",
    "- A **computational graph** is a way to represent mathematical functions using graph theory.  \n",
    "- Nodes represent **operations or variables**.  \n",
    "- Edges represent the **data flow** (values or weights) between nodes.  \n",
    "\n",
    "**Illustration:**  \n",
    "Equation:  \n",
    "\\[\n",
    "s = 2x\n",
    "\\]  \n",
    "\\[\n",
    "y = s + a\n",
    "\\]  \n",
    "\n",
    "- The computational graph has nodes for `x`, `s`, `a`, and `y`.  \n",
    "- Edges show how data flows from input → operations → output.  \n",
    "- Forward propagation: compute values from inputs to predict output using the graph.  \n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation\n",
    "- **Backpropagation** is an algorithm used to train neural networks by **propagating errors backward** to update weights/parameters.  \n",
    "- It uses the **chain rule** of derivatives to compute how the loss changes with respect to each parameter.  \n",
    "\n",
    "**Example:** Logistic Regression  \n",
    "\n",
    "- Inputs: \\(x_1, x_2\\)  \n",
    "- Weighted sum + bias: \\(z = w_1 x_1 + w_2 x_2 + b\\)  \n",
    "- Activation: \\(\\hat{y} = \\sigma(z)\\) (sigmoid function)  \n",
    "- Label: \\(y\\)  \n",
    "- Loss: Binary Cross-Entropy  \n",
    "\\[\n",
    "\\mathcal{L}(y, \\hat{y}) = -\\left[y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})\\right]\n",
    "\\]  \n",
    "\n",
    "**Backpropagation Steps:**  \n",
    "1. Compute **forward pass** to get predicted output \\(\\hat{y}\\).  \n",
    "2. Compute **loss** using the actual label \\(y\\).  \n",
    "3. Compute **gradients** of loss w.r.t weights using chain rule:  \n",
    "   \\[\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}\n",
    "   \\]  \n",
    "4. Update weights:  \n",
    "\\[\n",
    "w_i = w_i - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_i}\n",
    "\\]  \n",
    "where \\(\\eta\\) is the learning rate.  \n",
    "\n",
    "- Repeat steps for all training examples until convergence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
